{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projappl/project_2006327/Detectron/datasets/2024\n"
     ]
    }
   ],
   "source": [
    "%cd /projappl/project_2006327/Detectron/datasets/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "o5COPf_G_8VV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.logger import setup_logger\n",
    "logger = setup_logger()\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pickle \n",
    "import random\n",
    "import detectron2\n",
    "import numpy as np\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.config import CfgNode\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "\n",
    "# Suppress some user warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/06 09:22:00 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from output/faster_rcnn_object_detector_hook/model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "# Load your configuration\n",
    "cfg = get_cfg()\n",
    "\n",
    "#set directory for the best model\n",
    "output_dir = \"output/faster_rcnn_object_detector_hook\"\n",
    "cfg.OUTPUT_DIR = output_dir\n",
    "\n",
    "#load the configuration from cfg.pickle\n",
    "with open (\"output/faster_rcnn_object_detector_hook/cfg.pickle\", \"rb\") as f:\n",
    "    cfg = pickle.load(f)\n",
    "    cfg = CfgNode(cfg)\n",
    "    \n",
    "#load the model weight\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_best.pth\")\n",
    "\n",
    "#create a predictor using the loaded configuration\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some configurations for train, val and test\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "#directory\n",
    "name_ds = \"coco_data\"\n",
    "name_ds_test = name_ds + \"_test\"\n",
    "image_root_test = name_ds + \"/test\"\n",
    "#annotations\n",
    "af = \"_annotations.coco.json\"\n",
    "json_file_test = name_ds + \"/test/\" + af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test dataset\n",
    "register_coco_instances(\n",
    "    name = name_ds_test,\n",
    "    metadata = {},\n",
    "    json_file = json_file_test,\n",
    "    image_root = image_root_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the dataset\n",
    "cfg.DATASETS.TEST = (name_ds_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using test dataset: coco_data_test\n"
     ]
    }
   ],
   "source": [
    "#Access the test data from the cfg file- pickle file\n",
    "coco_data_test = cfg.DATASETS.TEST[0]\n",
    "print(f\"Using test dataset: {coco_data_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[02/06 09:22:32 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[02/06 09:22:32 d2.data.datasets.coco]: \u001b[0mLoaded 277 images in COCO format from coco_data/test/_annotations.coco.json\n",
      "\u001b[32m[02/06 09:22:32 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|     C      | 2642         |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[02/06 09:22:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[02/06 09:22:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[02/06 09:22:32 d2.data.common]: \u001b[0mSerializing 277 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/06 09:22:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.22 MiB\n",
      "\u001b[32m[02/06 09:22:32 d2.evaluation.evaluator]: \u001b[0mStart inference on 277 batches\n",
      "\u001b[32m[02/06 09:22:42 d2.evaluation.evaluator]: \u001b[0mInference done 1/277. Dataloading: 1.0310 s/iter. Inference: 8.8396 s/iter. Eval: 0.0003 s/iter. Total: 9.8713 s/iter. ETA=0:45:24\n",
      "\u001b[32m[02/06 09:22:47 d2.evaluation.evaluator]: \u001b[0mInference done 134/277. Dataloading: 0.0029 s/iter. Inference: 0.0342 s/iter. Eval: 0.0003 s/iter. Total: 0.0375 s/iter. ETA=0:00:05\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.evaluator]: \u001b[0mInference done 270/277. Dataloading: 0.0034 s/iter. Inference: 0.0335 s/iter. Eval: 0.0003 s/iter. Total: 0.0372 s/iter. ETA=0:00:00\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:10.125717 (0.037227 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:09 (0.033465 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.07 seconds.\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.169\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.558\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.169\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.039\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.188\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.271\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.271\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 16.916 | 55.794 | 4.109  | 16.916 |  nan  |  nan  |\n",
      "\u001b[32m[02/06 09:22:52 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Precision: 16.916325778792206\n"
     ]
    }
   ],
   "source": [
    "# Load your configuration\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_best.pth\")  # Load your trained model weights\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25  # Set a threshold for detections\n",
    "\n",
    "# Create an evaluator\n",
    "evaluator = COCOEvaluator(\"coco_data_test\", cfg, False, output_dir=\"./output\")\n",
    "\n",
    "# Load the test data loader\n",
    "test_loader = build_detection_test_loader(cfg, \"coco_data_test\")\n",
    "\n",
    "# Perform inference on the test dataset\n",
    "results = inference_on_dataset(predictor.model, test_loader, evaluator)\n",
    "\n",
    "# Calculate precision and recall from the evaluator\n",
    "precision = results[\"bbox\"][\"AP\"]\n",
    "#recall = results[\"bbox\"][\"AR\"]\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "#print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
